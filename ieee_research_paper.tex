\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Evaluating Legal Hallucinations in LLMs: A Comparative Analysis of the IPC to BNS Transition\\}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Harshal Patel}
\IEEEauthorblockA{\textit{Apex Institute of Technology} \\
\textit{Chandigarh University}\\
Mohali, India \\
hp842484n@gmail.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Aniruddh Agrahari}
\IEEEauthorblockA{\textit{Apex Institute of Technology} \\
\textit{Chandigarh University}\\
Mohali, India \\
aniruddhagrahari1@gmail.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Priya Karn}
\IEEEauthorblockA{\textit{Apex Institute of Technology} \\
\textit{Chandigarh University}\\
Mohali, India \\
karnpriya721@gmail.com}
\and
\IEEEauthorblockN{4\textsuperscript{th} Aryan}
\IEEEauthorblockA{\textit{Apex Institute of Technology} \\
\textit{Chandigarh University}\\
Mohali, India \\
aryan96@gmail.com}
}

\maketitle

\begin{abstract}
As of early 2026, the Bharatiya Nyaya Sanhita (BNS) has superseded the 1860 Indian Penal Code (IPC), fundamentally restructuring Indian criminal jurisprudence. However, "Legacy Bias"—the persistence of outdated statutory knowledge stemming from historical training corpora—remains a critical failure mode in Large Language Models (LLMs) deployed for legal informatics. This comprehensive study benchmarks eight leading foundational models against the IPC-to-BNS transition using a novel, highly proprietary dataset of 100 transitional legal scenarios. Moving beyond surface-level accuracy, we introduce a multidimensional penalization framework comprising Legal Claim Truthfulness (LCT), Substantive Groundedness (SGG), Abstention \& Calibration Rate (ACR), and crucially, the Extrinsic Citation Hallucination Rate (ECHR). We unify these metrics into the LegalBench Adjusted Score (LBAS). Evaluating 800 discrete legal inferences, our findings reveal a stark divergence in architectural safety thresholds. While models such as ChatGPT 5.2 achieve 80.0\% raw truthfulness, their 17.0\% hallucination rate renders them unsafe for unguarded legal deployment. Conversely, Gemini 3 exhibits unprecedented calibration, minimizing extrinsic hallucinations to 6.0\% and leading the LBAS index (77.5). Furthermore, our qualitative ablation studies demonstrate that dense parameter scaling without targeted reinforcement learning fails to mitigate historical overfitting, heavily penalizing open-weights models like Meta AI.
\end{abstract}

\begin{IEEEkeywords}
Legal Informatics, Generative AI, Hallucination Detection, LLM Benchmarking, IPC to BNS transition, Trustworthy AI, LegalBench, Indian Law.
\end{IEEEkeywords}

\section{Introduction}
The transition from the colonial-era Indian Penal Code (IPC) of 1860 to the Bharatiya Nyaya Sanhita (BNS) in late 2023 represents the most comprehensive legislative overhaul in modern Indian history. This transition is not merely a semantic renaming; it involves the complex renumbering, consolidation, modification, and repeal of hundreds of statutes that have formed the backbone of Indian legal texts for over 160 years. 

Simultaneously, the integration of Large Language Models (LLMs) into the legal technology sector has accelerated rapidly. Legal professionals increasingly rely on generative AI for case law summarization, statutory retrieval, and preliminary legal drafting. However, the foundational models driving these applications are constrained by their training data. We define the resulting vulnerability as "Data Inertia" or "Legacy Bias." 

A model trained on terabytes of historical Supreme Court of India judgments will naturally encode strong statistical associations between specific crimes and their legacy IPC sections (e.g., associating "Murder" immutably with "Section 302 IPC"). When queried about current BNS law (where Murder is now governed by Section 103 BNS), an LLM must override billions of highly weighted parameters to output the correct, newly established statute. Failure to do so results in a legal hallucination—specifically, an \textit{Extrinsic Citation Hallucination}—where the model provides legally obsolete or fabricated advice with high confidence.

The primary objective of this research is to rigorously quantify the degree to which state-of-the-art LLMs in early 2026 have mitigated Legacy Bias. We hypothesize that raw parameter scale is insufficient to overcome historical overfitting and that models employing specialized architectural safeguards or continuous temporal fine-tuning will demonstrate significantly lower hallucination rates.

\section{Literature Review}
The evaluation of LLMs within specialized, high-stakes domains has evolved significantly from generic natural language processing metrics. 

Generic benchmarks such as MMLU (Massive Multitask Language Understanding) often fail to capture the nuanced reasoning required in professional law. Guha et al. \cite{b_legalbench} introduced \textit{LegalBench}, a collaboratively constructed framework comprising 162 tasks that evaluate an LLM's capacity to perform practical legal reasoning, from contract rule extraction to statutory interpretation. Similarly, \textit{LawBench} \cite{b_lawbench} assesses cognitive levels spanning knowledge memorization to legal application. While these frameworks provide essential baseline metrics, they predominantly test spatial and logical reasoning within static legal snapshots. Our work extends this by evaluating \textit{temporal} statutory adaptation—the model's agility in unlearning deprecated laws.

Hallucination refers to generative outputs that are fluent but factually unfabricated or nonsensical \cite{b_ji2023survey}. In legal and medical contexts, researchers distinguish between intrinsic hallucinations (contradicting the user's prompt) and extrinsic hallucinations (generating unverifiable or false external facts, such as fake case citations or imaginary statutes). Specialized datasets like \textit{LegalHalBench} \cite{b_legalhal} and \textit{FalseCite} \cite{b_falsecite} have been developed to measure these phenomena. This paper specifically focuses on Extrinsic Citation Hallucinations (ECHR) triggered by historical semantic interference.

\sectionsection{Methodology}
Evaluating the IPC-to-BNS transition requires a rigorous methodological pipeline, encompassing data curation, standardized processing, model selection, and quantifiable metric synthesis.

\subsection{Dataset}
We developed the \textit{IndoLegal-100} dataset, comprising 100 expertly curated legal questions purposefully designed to trigger Legacy Bias. The dataset is segmented into four primary categories, reflecting the structural taxonomy of the BNS:
\begin{itemize}
    \item \textbf{Type A: Direct Renumbering (35\%):} Offenses that remained substantially unchanged in legal definition but were assigned entirely new section numbers. This tests basic temporal adaptation and fact retrieval. (e.g., IPC Sec 420 $\rightarrow$ BNS Sec 318).
    \item \textbf{Type B: Structural Mergers (25\%):} Scenarios where multiple related IPC offenses were consolidated into a single BNS provision. This tests the LLM's capacity to synthesize and understand broader legislative intent.
    \item \textbf{Type C: Novel Provisions \& Post-2024 Amendments (20\%):} Inquiries regarding statutes newly introduced by the BNS (e.g., specific provisions for organized crime or deceitful promises to marry). Models overly reliant on historical distributions often fail these completely.
    \item \textbf{Type D: Omission and Repeal (20\%):} Queries concerning well-known IPC sections (e.g., Sedition under 124A or Unnatural Offenses under 377) that have no direct, 1:1 equivalent in the BNS. These serve as trap queries to measure the Abstention \& Calibration Rate.
\end{itemize}

\subsection{Data Preprocessing}
Each model was dynamically evaluated against the 100 IndoLegal dataset questions utilizing a standardized system prompt designed to enforce rigorous academic and statutory formatting. We employed a basic Zero-Shot methodology devoid of Retrieval-Augmented Generation (RAG) components. By isolating the models from external search or vector databases, we explicitly measure the intrinsic, internalized statutory weights of the pre-trained and post-trained networks. The generative outputs were systematically parsed and logged into continuous JSON schema structures for batch grading.

\subsection{Model Architecture}
To ensure a comprehensive analysis of the early 2026 AI landscape, we evaluated 8 distinct foundational and fine-tuned models accessed via official APIs. The cohort spans global proprietary giants, highly efficient reasoning models, localized fine-tunes, and dense open-weights architectures.
\begin{itemize}
    \item \textbf{Proprietary Tier-1:} Gemini 3 (Google), ChatGPT 5.2 (OpenAI), Claude Sonnet 4.6 (Anthropic).
    \item \textbf{Reasoning Architectures:} DeepSeek V3.2.
    \item \textbf{Open-Weights Dense:} Meta AI, Grok 4.1.
    \item \textbf{Regional/Specialized:} Indus Sarvam, Kruti (tailored for Indic languages and regional contexts).
\end{itemize}

\subsection{Evaluation Metrics}
Accuracy in a legal context is not binary. A partially correct premise can be useful for research, while a confident but entirely hallucinated citation constitutes a catastrophic failure in legal practice. We adopt and refine five evaluation metrics to capture this continuum:

\begin{enumerate}
    \item \textbf{Legal Claim Truthfulness (LCT):} The overarching percentage of responses that are factually sound and pinpoint the correct primary statute.
    $LCT = (N_{Truthful}/N_{Total}) \times 100$
    
    \item \textbf{Substantive Groundedness \& Granularity (SGG):} The frequency at which the model correctly identifies the broader legal principle but fails to specify the exact statutory subunit or clause. High SGG indicates a model that possesses updated general knowledge but lacks high-resolution precision.

    \item \textbf{Extrinsic Citation Hallucination Rate (ECHR):} A critical safety metric defining instances where the LLM asserts a legal fact supported by a fabricated BNS citation, or stubbornly insists that an obsolete IPC section is active law.
    $ECHR = (N_{Hallucinated}/N_{Total}) \times 100$

    \item \textbf{Abstention \& Calibration Rate (ACR):} The rate at which the model correctly recognizes epistemic uncertainty and abstains rather than generating a hallucinated response.
    $ACR = (N_{Abstention}/N_{Total}) \times 100$

    \item \textbf{LegalBench Adjusted Score (LBAS):} A weighted composite index (0-100) combining the above parameters. It heavily penalizes hallucinations while forgiving safe abstentions.
    \begin{equation}
    LBAS_{raw} = (LCT \times 1.0) + (SGG \times 0.5) - (ECHR \times 1.0)
    \end{equation}
    Models exhibiting a negative $LBAS_{raw}$ are floored at an LBAS of 0, signifying absolute unreliability for legal tasks.
\end{enumerate}

\section{Results and Discussion}
The benchmarking harness successfully captured 800 independent evaluations. The performance data was extracted, sanitized, and classified according to the LBAS framework.

\subsection{Training and Validation Performance (Overall Benchmark)}
Table \ref{tab_empirical_results} details the empirical performance of the eight evaluated models according to the proposed LBAS methodology on our standardized test set.

\begin{table}[htbp]
\caption{Overall BNS Benchmarking Results (N=100 per model)}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|P{0.8cm}|P{0.8cm}|P{0.8cm}|P{0.8cm}|P{0.9cm}|}
\hline
\textbf{Model Identifier} & \textbf{LCT (\%)} & \textbf{ECHR (\%)} & \textbf{SGG (\%)} & \textbf{ACR (\%)} & \textbf{LBAS} \\
\hline
\textbf{Gemini 3} & 73.0 & \textbf{6.0} & 21.0 & 0.0 & \textbf{77.5} \\
ChatGPT 5.2 & \textbf{80.0} & 17.0 & 3.0 & 0.0 & 64.5 \\
Indus Sarvam & 79.0 & 19.0 & 2.0 & 0.0 & 61.0 \\
Grok 4.1 & 69.0 & 25.0 & 5.0 & 1.0 & 46.5 \\
Claude Sonnet 4.6 & 63.0 & 28.0 & 9.0 & 0.0 & 39.5 \\
DeepSeek V3.2 & 66.0 & 29.0 & 5.0 & 0.0 & 39.5 \\
Kruti & 44.0 & 53.0 & 2.0 & 1.0 & 0.0 \\
Meta AI & 22.0 & 76.0 & 2.0 & 0.0 & 0.0 \\
\hline
\end{tabular}
\label{tab_empirical_results}
\end{center}
\end{table}

\subsection{Test Set Evaluation (Diverging Reliability)}
As detailed in Table \ref{tab_empirical_results}, evaluating models strictly by raw accuracy paints a dangerously incomplete picture. \textbf{ChatGPT 5.2} achieved the highest absolute Truthfulness (LCT) at 80.0\%. However, in the 20\% of scenarios where it failed, it almost exclusively generated Extrinsic Hallucinations (ECHR: 17.0\%), confidently citing fabricated BNS sub-clauses. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{charts/fig1_diverging_reliability.png}
    \caption{Diverging Assessment of Legal Reliability. Positive components (Green/Blue) indicating LCT and SGG extend to the right, whereas penalizing ECHR hallucinations (Red) extend to the left.}
    \label{fig:diverging}
\end{figure}

Figure \ref{fig:diverging} visually elucidates the severity of Legacy Bias in lower-performing architectures. Models like Kruti (ECHR: 53.0\%) and Meta AI (ECHR: 76.0\%) sustain extreme penalizations. Because their negative hallucination rates vastly numerically surpass their positive truthful responses, their net calculations fall below zero, mathematically proving they are structurally unsafe for unassisted legal retrieval tasks.

\subsection{Classification Metrics (Multidimensional Competency)}
Evaluating multifaceted AI behavior requires multi-axis spatial visualization.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{charts/fig2_radar_competency.png}
    \caption{Multidimensional Legal Competency Radar Chart focusing on the elite quartile of models. ECHR is inverted ($100 - ECHR$) to denote Safety.}
    \label{fig:radar}
\end{figure}

The radar chart (Figure \ref{fig:radar}) underscores the distinct behavioral modalities of the superior models. \textbf{Indus Sarvam}, a model localized to Indian legal sub-dialects, closely tracks ChatGPT 5.2 in structural mapping. However, Gemini 3's distinct expansion on the Safety and Groundedness axes clearly demarcates its specialized optimization against false legal confidence.

\subsection{Discussion}
To crystallize the quantitative metrics, we conduct qualitative ablation studies on varying IPC-to-BNS friction points.

\textbf{Case Study A: The "Section 420" Overfit.} Under the IPC, Section 420 (Cheating) transcended legal jargon to become a cultural norm. Meta AI consistently suffered from Extrinsic Hallucination in this area, stating, "Under Section 420 of the BNS, cheating is punishable by...". The LLM's attention mechanism fails to overcome the immense hyper-parameter weight assigned to the historical '420 $\rightarrow$ Cheating' token correlation. Conversely, Indus Sarvam successfully executed temporal unlearning, correctly mapping the provision to Section 318 BNS.

\textbf{Case Study B: Omitted Statutes.} Sedition (Section 124A IPC) was omitted in name from the BNS. When faced with this trap question, Grok 4.1 succumbed and fabricated "Section 147(B) BNS for Sedition," resulting in severe ECHR penalization. 

The empirical evidence suggests that architectural scale (parameter count) does not linearly correlate with legislative agility. Meta AI and Grok 4.1 are massive, highly capable conversational models, yet they succumb readily to Extrinsic Citation Hallucinations when deployed in a zero-shot legal environment. This research indicates that \textbf{Continuous Temporal Fine-Tuning} and rigorous \textbf{Reinforcement Learning from Human Feedback (RLHF)} applied specifically to negative legal constraints are mandatory. 

\subsection{Conclusion of Results}
The evaluation framework decisively establishes the operational realities of adopting generative systems during sovereign statutory transitions. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{charts/fig3_lbas_rankings.png}
    \caption{Final LegalBench Adjusted Score (LBAS) index rankings.}
    \label{fig:ranking}
\end{figure}

\textbf{Gemini 3} proved to be a vastly more calibrated architecture for legal deployment. It attained an LCT of 73.0\%, but dramatically minimized its ECHR to a remarkable 6.0\%. Furthermore, Gemini 3 exhibited a high Groundedness rate (SGG: 21.0\%). When Gemini 3 could not identify the specific millimeter-level sub-clause, it provided the overarching chapter and broad section number, refusing to hallucinate a false sub-section. This robust safety threshold propels Gemini 3 to the paramount position on the LBAS index (77.5). The steep performance drop observed across open-weights models (Figure \ref{fig:ranking}) illustrates that standard conversational LLMs remain highly volatile without secondary validation protocols.

\section{Future Scope}
This benchmarking study was executed utilizing a zero-shot inference protocol entirely dependent on the models' parametric memory architectures. Real-world legal tech applications predominantly employ Retrieval-Augmented Generation (RAG) pipelines, grounding the LLM via semantic similarity searches over authoritative, up-to-date vector databases containing the literal BNS text. 

Future research must evaluate whether a RAG implementation successfully nullifies Legacy Bias for lower-scoring models like Meta AI, or whether the deeply ingrained IPC token correlations override the retrieved contextual window—a phenomenon known in the literature as "Contextual Rejection." Furthermore, expanding the IndoLegal dataset from 100 to over 5,000 algorithmic variations across specific State Amendments represents the immediate horizon for this benchmarking framework.

\section*{Acknowledgment}
The authors express their profound gratitude to the Apex Institute of Technology at Chandigarh University for providing the computational infrastructure allocations requisite to execute this extensive legal evaluation matrix and compile the visualizations.

\begin{thebibliography}{00}
\bibitem{bma_bns} Ministry of Home Affairs, Government of India, "The Bharatiya Nyaya Sanhita, 2023," \textit{The Gazette of India}, CG-DL-E-25122023-250883, Dec. 2023.
\bibitem{b_legalbench} N. Guha, et al., "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models," \textit{Advances in Neural Information Processing Systems}, vol. 36, 2024.
\bibitem{b_lawbench} Z. Fei, et al., "LawBench: Benchmarking Legal Knowledge of Large Language Models," \textit{arXiv preprint arXiv:2309.16289}, 2023.
\bibitem{b_ji2023survey} Z. Ji, et al., "Survey of Hallucination in Natural Language Generation," \textit{ACM Computing Surveys}, vol. 55, no. 12, pp. 1-38, 2023.
\bibitem{b_legalhal} D. Yue, et al., "LegalHalBench: A Benchmark for Evaluating Legal Hallucinations in Large Language Models," \textit{arXiv preprint arXiv:2408.06822}, 2024.
\bibitem{b_falsecite} Y. Zhong, et al., "FalseCite: Benchmarking Hallucinated Citations in Legal Large Language Models," \textit{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics}, 2024.
\bibitem{b_rlhf} L. Ouyang, et al., "Training language models to follow instructions with human feedback," \textit{Advances in Neural Information Processing Systems}, vol. 35, pp. 27730--27744, 2022.
\end{thebibliography}

\end{document}
